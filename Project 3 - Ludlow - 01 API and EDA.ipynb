{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Reddit API Classification & Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tom Ludlow, DSI-NY-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NLP to identify posts from **r/audioengineering** and **r/livesound**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project file contains 5 notebooks covering the data science process for the following problem:\n",
    "\n",
    "### How well can Natural Language Processing models differentiate post content from two similar subreddits, and which type of Classifier works best?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: API and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code used to query the Reddit API and store post data from our target subreddits **r/AudioEngineering** and **r/LiveAudio**.  This code performed 10 pulls to collect the maximum of 1000 available posts per subreddit.  These posts are then formatted and prepared to save and use for the remainder of the project.\n",
    "\n",
    "### Contents:\n",
    "- [**Loop to pull Reddit API posts**](#Loop-to-pull-Reddit-API-posts)\n",
    "- [**EDA**](#EDA)\n",
    "    -  [Formatting changes](#Formatting-changes-after-loading-from-files)\n",
    "    -  [Unravel posts](#Unravel-posts)\n",
    "- [**Save separate DataFrames**](#Save-separate-DataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import ast # to convert string data to indexable list of dictionaries\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random state var\n",
    "r = 1219"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop to pull Reddit API posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect our Reddit data, we used the existing `.json` API format, which returns a dictionary for URLs containing a `.json` extension.  We include a headers dictionary to identify ourselves to Reddit, which allows us to execute an API loop to accumulate the maximum allowed posts (~1000 per subreddit).\n",
    "\n",
    "Once this data is scraped and obtained, we saved the raw contents to a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create header parameter for API\n",
    "headers_dict = {'User-agent':'twludlow'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 200\n",
      "sub01_pages length:  1\n",
      "sub01_after:  t3_a3vakv\n",
      "0 200\n",
      "sub02_pages length:  1\n",
      "sub02_after:  t3_a32xoh\n",
      "https://reddit.com/r/audioengineering.json?limit=100&after=t3_a3vakv\n",
      "https://reddit.com/r/livesound.json?limit=100&after=t3_a32xoh\n",
      "1 200\n",
      "sub01_pages length:  2\n",
      "sub01_after:  t3_a1a83f\n",
      "1 200\n",
      "sub02_pages length:  2\n",
      "sub02_after:  t3_9zp2tb\n",
      "https://reddit.com/r/audioengineering.json?limit=100&after=t3_a1a83f\n",
      "https://reddit.com/r/livesound.json?limit=100&after=t3_9zp2tb\n",
      "2 200\n",
      "sub01_pages length:  3\n",
      "sub01_after:  t3_9yjhye\n",
      "2 200\n",
      "sub02_pages length:  3\n",
      "sub02_after:  t3_9xp13m\n",
      "https://reddit.com/r/audioengineering.json?limit=100&after=t3_9yjhye\n",
      "https://reddit.com/r/livesound.json?limit=100&after=t3_9xp13m\n",
      "3 200\n",
      "sub01_pages length:  4\n",
      "sub01_after:  t3_9wbrxt\n",
      "3 200\n",
      "sub02_pages length:  4\n",
      "sub02_after:  t3_9vrhrc\n",
      "https://reddit.com/r/audioengineering.json?limit=100&after=t3_9wbrxt\n",
      "https://reddit.com/r/livesound.json?limit=100&after=t3_9vrhrc\n",
      "4 200\n",
      "sub01_pages length:  5\n",
      "sub01_after:  t3_9tusd7\n",
      "4 200\n",
      "sub02_pages length:  5\n",
      "sub02_after:  t3_9ssc53\n",
      "https://reddit.com/r/audioengineering.json?limit=100&after=t3_9tusd7\n",
      "https://reddit.com/r/livesound.json?limit=100&after=t3_9ssc53\n",
      "5 200\n",
      "sub01_pages length:  6\n",
      "sub01_after:  t3_9r1q2t\n",
      "5 200\n",
      "sub02_pages length:  6\n",
      "sub02_after:  t3_9pejtv\n",
      "https://reddit.com/r/audioengineering.json?limit=100&after=t3_9r1q2t\n",
      "https://reddit.com/r/livesound.json?limit=100&after=t3_9pejtv\n",
      "6 200\n",
      "sub01_pages length:  7\n",
      "sub01_after:  t3_9p1jlj\n",
      "6 200\n",
      "sub02_pages length:  7\n",
      "sub02_after:  t3_9n4etx\n",
      "https://reddit.com/r/audioengineering.json?limit=100&after=t3_9p1jlj\n",
      "https://reddit.com/r/livesound.json?limit=100&after=t3_9n4etx\n",
      "7 200\n",
      "sub01_pages length:  8\n",
      "sub01_after:  t3_9mj07r\n",
      "7 200\n",
      "sub02_pages length:  8\n",
      "sub02_after:  t3_9kg568\n",
      "https://reddit.com/r/audioengineering.json?limit=100&after=t3_9mj07r\n",
      "https://reddit.com/r/livesound.json?limit=100&after=t3_9kg568\n",
      "8 200\n",
      "sub01_pages length:  9\n",
      "sub01_after:  t3_9kcfjh\n",
      "8 200\n",
      "sub02_pages length:  9\n",
      "sub02_after:  t3_9i15s7\n",
      "https://reddit.com/r/audioengineering.json?limit=100&after=t3_9kcfjh\n",
      "https://reddit.com/r/livesound.json?limit=100&after=t3_9i15s7\n",
      "9 200\n",
      "sub01_pages length:  10\n",
      "sub01_after:  None\n",
      "9 200\n",
      "sub02_pages length:  10\n",
      "sub02_after:  None\n"
     ]
    }
   ],
   "source": [
    "# instantiate API variables\n",
    "url = 'https://reddit.com/'\n",
    "sub01_url = url + 'r/audioengineering' # set sub01 to 'Audio Engineering'\n",
    "sub02_url = url + 'r/livesound'        # set sub02 to 'Live Sound'\n",
    "\n",
    "limit_num = 100     # API 'limit' parameter\n",
    "\n",
    "sub01_after = None  # instantiate empty counters for API 'after' parameter\n",
    "sub02_after = None\n",
    "\n",
    "sub01_pages = []    # instantiate empty lists to save API results\n",
    "sub02_pages = []\n",
    "\n",
    "for i in range(10): # pull from API 20 times\n",
    "    \n",
    "    # add 'after' parameters if an id has been saved - starts as None\n",
    "    if sub01_after and sub02_after:\n",
    "        # create full API url for sub01\n",
    "        sub01_after_url = sub01_url + '.json?limit=' \\\n",
    "                            + str(limit_num) + '&after=' \\\n",
    "                            + sub01_after\n",
    "        print(sub01_after_url)\n",
    "        \n",
    "        # create full API url for sub02\n",
    "        sub02_after_url = sub02_url + '.json?limit=' \\\n",
    "                            + str(limit_num) + '&after=' \\\n",
    "                            + sub02_after\n",
    "        print(sub02_after_url)\n",
    "    \n",
    "    # if one after is logged and the other is not\n",
    "    elif bool(sub01_after) != bool(sub02_after):\n",
    "        print('After reference out of sync.')\n",
    "        break\n",
    "    \n",
    "    else:\n",
    "        # create first run url\n",
    "        sub01_after_url = sub01_url + '.json?limit=' + str(limit_num)\n",
    "        sub02_after_url = sub02_url + '.json?limit=' + str(limit_num)\n",
    "    \n",
    "    # pull json from sub01\n",
    "    sub01_res = requests.get(sub01_after_url, headers=headers_dict)\n",
    "    print(i, sub01_res.status_code)\n",
    "    \n",
    "    # if sub01 connection is established\n",
    "    if sub01_res.status_code == 200:\n",
    "        # add page to list\n",
    "        sub01_pages.append(sub01_res.json()['data'])\n",
    "        print('sub01_pages length: ', len(sub01_pages))\n",
    "        \n",
    "        # set 'after' parameter for next run\n",
    "        sub01_after = sub01_res.json()['data']['after']\n",
    "        print('sub01_after: ', sub01_after)\n",
    "        \n",
    "    else:        \n",
    "        print('Connection failed.\\n')\n",
    "    \n",
    "    # sleep one second\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # pull json from sub02\n",
    "    sub02_res = requests.get(sub02_after_url, headers=headers_dict)\n",
    "    print(i, sub02_res.status_code)\n",
    "    \n",
    "    # if sub02 connection is established\n",
    "    if sub02_res.status_code == 200:\n",
    "        # add page to list\n",
    "        sub02_pages.append(sub02_res.json()['data'])\n",
    "        print('sub02_pages length: ', len(sub02_pages))\n",
    "        \n",
    "        # set 'after' parameter for next run\n",
    "        sub02_after = sub02_res.json()['data']['after']\n",
    "        print('sub02_after: ', sub02_after)\n",
    "    else:\n",
    "        print('Connection failed.\\n')\n",
    "        \n",
    "    # sleep one second    \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrames from posting lists\n",
    "sub01_df = pd.DataFrame(sub01_pages)\n",
    "sub02_df = pd.DataFrame(sub02_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save API data to files\n",
    "sub01_df.to_csv('./audio_engineering_posts.csv', index=False)\n",
    "sub02_df.to_csv('./live_sound_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves: 181214_audioengineering_posts.csv, 181214_live_sound_posts.csv\n",
    "\n",
    "Iterations: **10**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the format of the data we've obtained: a dictionary, with post data contained in the `children` key.  Within this, we will need to access the nested keys below. \n",
    "\n",
    "_(Screenshot used to adjust original pull results display.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/reddit_api_pull_screen.png\" alt=\"Result header from initial Reddit API Pull\" title=\"Reddit API JSON Header\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload original API data from our subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub01_df = pd.read_csv('./reddit_data/181214_audio_engineering_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub02_df = pd.read_csv('./reddit_data/181214_live_sound_posts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `ast` library function `literal_eval` takes input of type `string` and processes it as if it were Python syntax.  In this case, it is converting strings of dictionaries back into dictionaries stored in active notebook memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub01_df['children'] = sub01_df.children.map(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub02_df['children'] = sub02_df.children.map(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting changes after loading from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>after</th>\n",
       "      <th>before</th>\n",
       "      <th>children</th>\n",
       "      <th>dist</th>\n",
       "      <th>modhash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_a3vakv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'kind': 't3', 'data': {'approved_at_utc': No...</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_a1a83f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'kind': 't3', 'data': {'approved_at_utc': No...</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_9yjhye</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'kind': 't3', 'data': {'approved_at_utc': No...</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_9wbrxt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'kind': 't3', 'data': {'approved_at_utc': No...</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_9tusd7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'kind': 't3', 'data': {'approved_at_utc': No...</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       after  before                                           children  dist  \\\n",
       "0  t3_a3vakv     NaN  [{'kind': 't3', 'data': {'approved_at_utc': No...   102   \n",
       "1  t3_a1a83f     NaN  [{'kind': 't3', 'data': {'approved_at_utc': No...   100   \n",
       "2  t3_9yjhye     NaN  [{'kind': 't3', 'data': {'approved_at_utc': No...   100   \n",
       "3  t3_9wbrxt     NaN  [{'kind': 't3', 'data': {'approved_at_utc': No...   100   \n",
       "4  t3_9tusd7     NaN  [{'kind': 't3', 'data': {'approved_at_utc': No...   100   \n",
       "\n",
       "   modhash  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub01_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub01_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>after</th>\n",
       "      <th>before</th>\n",
       "      <th>children</th>\n",
       "      <th>dist</th>\n",
       "      <th>modhash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_a32xoh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'kind': 't3', 'data': {'approved_at_utc': No...</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_9zp2tb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'kind': 't3', 'data': {'approved_at_utc': No...</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_9xp13m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'kind': 't3', 'data': {'approved_at_utc': No...</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_9vrhrc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'kind': 't3', 'data': {'approved_at_utc': No...</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_9ssc53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'kind': 't3', 'data': {'approved_at_utc': No...</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       after  before                                           children  dist  \\\n",
       "0  t3_a32xoh     NaN  [{'kind': 't3', 'data': {'approved_at_utc': No...   102   \n",
       "1  t3_9zp2tb     NaN  [{'kind': 't3', 'data': {'approved_at_utc': No...   100   \n",
       "2  t3_9xp13m     NaN  [{'kind': 't3', 'data': {'approved_at_utc': No...   100   \n",
       "3  t3_9vrhrc     NaN  [{'kind': 't3', 'data': {'approved_at_utc': No...   100   \n",
       "4  t3_9ssc53     NaN  [{'kind': 't3', 'data': {'approved_at_utc': No...   100   \n",
       "\n",
       "   modhash  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub02_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub02_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save post dictionaries in arrays\n",
    "\n",
    "ae_posts_bulk = sub01_df['children']\n",
    "ls_posts_bulk = sub02_df['children']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [{'kind': 't3', 'data': {'approved_at_utc': No...\n",
       "1    [{'kind': 't3', 'data': {'approved_at_utc': No...\n",
       "2    [{'kind': 't3', 'data': {'approved_at_utc': No...\n",
       "3    [{'kind': 't3', 'data': {'approved_at_utc': No...\n",
       "4    [{'kind': 't3', 'data': {'approved_at_utc': No...\n",
       "Name: children, dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_posts_bulk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [{'kind': 't3', 'data': {'approved_at_utc': No...\n",
       "1    [{'kind': 't3', 'data': {'approved_at_utc': No...\n",
       "2    [{'kind': 't3', 'data': {'approved_at_utc': No...\n",
       "3    [{'kind': 't3', 'data': {'approved_at_utc': No...\n",
       "4    [{'kind': 't3', 'data': {'approved_at_utc': No...\n",
       "Name: children, dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_posts_bulk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_posts_bulk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "for post in ae_posts_bulk: \n",
    "    print(len(post))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unravel posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Title: 'title'\n",
    " - Posts: 'selftext'\n",
    " - Author: 'author_fullname'\n",
    " - Upvotes: 'ups'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will collect and store the bulleted data to perform our initial modeling analysis, and to leave room for additional supplemental analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tech Support and Troubleshooting - December 10, 2018'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_posts_bulk[0][0]['data']['title'] # format to isolate data for Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Welcome the /r/audioengineering Tech Support and Troubleshooting Thread.  We kindly ask that all tech support questions and basic troubleshooting questions (how do I hook up 'a' to 'b'?, headphones vs mons, etc) go here.  If you see posts that belong here, please report them to help us get to them in a timely manner.  Thank you!\\n\\n   Daily Threads:\\n\\n\\n* [Monday - Gear Recommendations Sticky Thread](http://www.reddit.com/r/audioengineering/search?q=title%3Arecommendation+author%3Aautomoderator&amp;restrict_sr=on&amp;sort=new&amp;t=all)\\n* [Monday - Tech Support and Troubleshooting Sticky Thread](http://www.reddit.com/r/audioengineering/search?q=title%3ASupport+author%3Aautomoderator&amp;restrict_sr=on&amp;sort=new&amp;t=all)\\n* [Tuesday - Tips &amp; Tricks](http://www.reddit.com/r/audioengineering/search?q=title%3A%22tuesdays%22+AND+%28author%3Aautomoderator+OR+author%3Ajaymz168%29&amp;restrict_sr=on&amp;sort=new&amp;t=all)\\n* [Friday - How did they do that?](http://www.reddit.com/r/audioengineering/search?q=title%3AFriday+author%3Aautomoderator&amp;restrict_sr=on&amp;sort=new&amp;t=all)\\n\""
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_posts_bulk[0][0]['data']['selftext'] # format to isolate data for Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Post the pictures you took at your gigs this week!'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_posts_bulk[0][0]['data']['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t2_6l4z3'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_posts_bulk[0][0]['data']['author_fullname'] # format to isolate author's name in base36 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t2_6l4z3'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_posts_bulk[0][0]['data']['author_fullname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_posts_bulk[0][0]['data']['ups'] # format to get number of upvotes on a post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post titles - 'title'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect post attributes, we used nested list comprehensions: for each subreddit post batch (sized 100 posts), iterate through each individual post dictionary and return the value from each attribute's key, (e.g. `title`,`selftext`,etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ae' for audio engineering\n",
    "ae_titles = [ae_posts_bulk[i][j]['data']['title'] for i in range(len(ae_posts_bulk))\n",
    "            for j in range(len(ae_posts_bulk[i]))]\n",
    "\n",
    "# 'ls' for live sound\n",
    "ls_titles = [ls_posts_bulk[i][j]['data']['title'] for i in range(len(ls_posts_bulk)) \n",
    "            for j in range(len(ls_posts_bulk[i]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posts - 'selftext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of post using nested comprehensions\n",
    "ae_posts = [ae_posts_bulk[i][j]['data']['selftext'] for i in range(len(ae_posts_bulk)) \n",
    "            for j in range(len(ae_posts_bulk[i]))]\n",
    "\n",
    "ls_posts = [ls_posts_bulk[i][j]['data']['selftext'] for i in range(len(ls_posts_bulk)) \n",
    "            for j in range(len(ls_posts_bulk[i]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "924"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ae_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "980"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ls_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upvotes - 'ups'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of upvotes using nested comprehensions\n",
    "ae_ups = [ae_posts_bulk[i][j]['data']['ups'] for i in range(len(ae_posts_bulk)) \n",
    "            for j in range(len(ae_posts_bulk[i]))]\n",
    "\n",
    "ls_ups = [ls_posts_bulk[i][j]['data']['ups'] for i in range(len(ls_posts_bulk)) \n",
    "            for j in range(len(ls_posts_bulk[i]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors - 'author_fullname'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing manually to handle missing author data.  When running the above comprehension, because of `null` values in the authors field, it failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_authors = [] # empty lists to store results\n",
    "ls_authors = []\n",
    "\n",
    "for i in range(len(ae_posts_bulk)): # for each bulk post (size 100)\n",
    "    for j in range(len(ae_posts_bulk[i])): # for each post in the batch\n",
    "        try:\n",
    "            ae_authors.append(ae_posts_bulk[i][j]['data']['author_fullname']) # attempt to add to list\n",
    "        except:\n",
    "            ae_authors.append('no author') # if it fails, add text stating 'no author'\n",
    "            \n",
    "for i in range(len(ls_posts_bulk)): # for each bulk post\n",
    "    for j in range(len(ls_posts_bulk[i])): # for each individual post\n",
    "        try:\n",
    "            ls_authors.append(ls_posts_bulk[i][j]['data']['author_fullname']) # attempt to add to list\n",
    "        except:\n",
    "            ls_authors.append('no author') # if it fails, add instead 'no author'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "924"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ae_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "980"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ls_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile lists into DataFrame\n",
    "ae_df = pd.DataFrame([ae_titles, ae_posts, ae_authors, ae_ups], index=['title','post','author','upvotes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose from rows to columns\n",
    "ae_df = ae_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile lists into DataFrame\n",
    "ls_df = pd.DataFrame([ls_titles, ls_posts, ls_authors, ls_ups], index=['title','post','author','upvotes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose from rows to columns\n",
    "ls_df = ls_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save separate DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_df.to_csv('./csv/ae_df.csv', index=False)\n",
    "ls_df.to_csv('./csv/ls_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data to csv above, then reload from csv below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_df = pd.read_csv('./csv/ae_df.csv')\n",
    "ls_df = pd.read_csv('./csv/ls_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our initial data tables, we designate each with a binary value for the status `'is_ls'`, where `0` is for all posts from r/audioengineering, and `1` for r/livesound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize our classifier: 'is_ls' (is live sound)\n",
    "ae_df['is_ls'] = 0\n",
    "ls_df['is_ls'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the classified posts in a single DataFrame, then replace all `NaN` values with `' '`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([ae_df, ls_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.post.fillna(' ', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our testing vector is a combination of `title` and `post` data, which we combined into the `comb` column as the title, a single space, then the post string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comb'] = df['title'] + ' ' + df['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>post</th>\n",
       "      <th>author</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>is_ls</th>\n",
       "      <th>comb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tech Support and Troubleshooting - December 10...</td>\n",
       "      <td>Welcome the /r/audioengineering Tech Support a...</td>\n",
       "      <td>t2_6l4z3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>Tech Support and Troubleshooting - December 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gear Recommendation (What Should I Buy?) Threa...</td>\n",
       "      <td>Welcome to our weekly Gear Recommendation Thre...</td>\n",
       "      <td>t2_6l4z3</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>Gear Recommendation (What Should I Buy?) Threa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Will I EVER understand compression...?</td>\n",
       "      <td>Ahh yes, my monthly compression post...\\n\\nI '...</td>\n",
       "      <td>t2_2r3uhjqr</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>Will I EVER understand compression...? Ahh yes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm interviewing to be an intern at a big stud...</td>\n",
       "      <td>What questions should I ask?\\n\\nEdit: I'm gett...</td>\n",
       "      <td>t2_dd3qi</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm interviewing to be an intern at a big stud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If I faced two speakers towards each other, on...</td>\n",
       "      <td></td>\n",
       "      <td>t2_bl2x2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>If I faced two speakers towards each other, on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Tech Support and Troubleshooting - December 10...   \n",
       "1  Gear Recommendation (What Should I Buy?) Threa...   \n",
       "2             Will I EVER understand compression...?   \n",
       "3  I'm interviewing to be an intern at a big stud...   \n",
       "4  If I faced two speakers towards each other, on...   \n",
       "\n",
       "                                                post       author  upvotes  \\\n",
       "0  Welcome the /r/audioengineering Tech Support a...     t2_6l4z3        7   \n",
       "1  Welcome to our weekly Gear Recommendation Thre...     t2_6l4z3       15   \n",
       "2  Ahh yes, my monthly compression post...\\n\\nI '...  t2_2r3uhjqr       96   \n",
       "3  What questions should I ask?\\n\\nEdit: I'm gett...     t2_dd3qi      145   \n",
       "4                                                        t2_bl2x2        5   \n",
       "\n",
       "   is_ls                                               comb  \n",
       "0      0  Tech Support and Troubleshooting - December 10...  \n",
       "1      0  Gear Recommendation (What Should I Buy?) Threa...  \n",
       "2      0  Will I EVER understand compression...? Ahh yes...  \n",
       "3      0  I'm interviewing to be an intern at a big stud...  \n",
       "4      0  If I faced two speakers towards each other, on...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      0\n",
       "post       0\n",
       "author     0\n",
       "upvotes    0\n",
       "is_ls      0\n",
       "comb       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1904, 6)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that among our `is_ls` classes, we have 980 posts from r/livesound, and 924 posts from r/audioengineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    980\n",
       "0    924\n",
       "Name: is_ls, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_ls.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = range(len(df)) # re-index, since original index values were preserved during concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for empty posts and store index to list\n",
    "to_drop = []\n",
    "\n",
    "for i, post in enumerate(df['comb']):\n",
    "    if len(post)==0:\n",
    "        to_drop.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with empty posts using index list\n",
    "df.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    980\n",
       "0    924\n",
       "Name: is_ls, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_ls.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./csv/181219_post_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This EDA process has created a DataFrame table containing titles, post, and combined values, as well as our target vector `is_ls`.  We have saved this to a csv file, and will proceed with pre-processing in Notebook 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue to Notebook 2: Pre-Processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
